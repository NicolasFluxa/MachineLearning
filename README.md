## üìå Claves de Relaci√≥n (IDs)

* **bookingid**: Identificador √∫nico num√©rico de la reserva.
* **passengerid**: Identificador √∫nico del pasajero dentro de una reserva.
* **recordlocator (PNR)**: C√≥digo alfanum√©rico del registro de reserva.
* **passenger_key**: Concatenaci√≥n del tipo de documento (Pasaporte/CI) + RUT o n√∫mero de pasaporte. Un pasajero puede tener m√∫ltiples reservas.

---

## üí∞ Revenue

* **total_revenue**: Ingresos totales generados por la reserva.
* **fare_revenue**: Ingresos correspondientes a la tarifa base.
* **ancillary_revenue**: Ingresos por servicios complementarios (equipaje, asientos, etc.).
* **discounted_fare_revenue_dc**: Ingresos por tarifa con descuento v√≠a c√≥digo de descuento.
* **discounted_fare_revenue_pc**: Ingresos por tarifa con descuento porcentual.
* **promocode**: C√≥digo promocional aplicado a la reserva (si existe).
* **has_promo_class**: Indicador (1/0) de si la reserva incluye una clase/tarifa promocional.

---

## üßç Pasajero (Flyer)

* **pax_type**: Tipo de pasajero (ADT = Adulto, CHD = Ni√±o, INF = Beb√©).
* **pax_gender**: Sexo del pasajero (1 = Hombre, 2 = Mujer).
* **language**: Idioma preferido del pasajero (si est√° disponible).

---

## üì¶ Booking ‚Äì Informaci√≥n Comercial (Booking 1)

* **pos**: Punto de venta de la reserva.
* **channelID**: Canal de venta

  * 1 = Contact Center
  * 2 = Website
  * 3 = API
  * 5 = GDS
* **channeltype**: Tipo de canal (Directo / Indirecto ‚Äì Agencias).
* **channel_detail**: Detalle espec√≠fico del canal de venta.
* **status**: Estado de la reserva (2, 3 = v√°lidos).

---

## üïí Booking ‚Äì Fechas y Tiempos

* **booking_date / booking_dt_utc**: Fecha y hora de emisi√≥n de la reserva (UTC).
* **time_zone**: Zona horaria asociada a la reserva.
* **booking_weeknumber**: Semana del a√±o en que se realiz√≥ la reserva.
* **booking_weekday / booking_dow**: D√≠a de la semana de compra (0‚Äì6 o 1‚Äì7).
* **booking_hour**: Hora del d√≠a en que se realiz√≥ la reserva.

---

## ‚úàÔ∏è Viaje ‚Äì Fechas Clave

* **tripstart_date**: Fecha de salida del primer vuelo.
* **tripend_date**: Fecha de regreso (si aplica).
* **tripstart_weekday**: D√≠a de la semana del inicio del viaje (0‚Äì6).
* **tripstart_month**: Mes de inicio del viaje (1‚Äì12).

---

## ‚è±Ô∏è M√©tricas de Tiempo (Calculadas)

* **days_to_departure / d2g (Days to Gate)**: D√≠as entre la compra y la salida.
* **los (Length of Stay)**: Duraci√≥n del viaje en d√≠as (null para OneWay).
* **advance_ratio**: Desconocida.

---

## üìä Bins y Categorizaciones

### Anticipaci√≥n de Compra

* **advance_bin**:

  * Alta: > 15 d√≠as
  * Media: 4‚Äì14 d√≠as
  * Baja: < 4 d√≠as
  * Negativos: null

### Duraci√≥n del Viaje

* **trip_length_bin**:

  * Largo: ‚â• 6 d√≠as
  * Medio: > 2 y < 6 d√≠as
  * Corto: ‚â§ 2 d√≠as

---

## üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Pasajeros en la Reserva (Booking 2)

* **paxs / passengers**: Total de pasajeros en la reserva.
* **adults**: N√∫mero de adultos.
* **children / childs**: N√∫mero de ni√±os (con asiento).
* **infants**: N√∫mero de beb√©s (en brazos).
* **has_chd_inf / has_child / has_infant**: Indicadores (1/0).
* **is_group**: Indicador (1/0) de reserva grupal.

---

## üåç Caracter√≠sticas del Viaje

* **trip_type**: C√≥digo num√©rico del tipo de viaje.
* **trip_rt**: Descripci√≥n del tipo de viaje (RoundTrip / OneWay).
* **is_roundtrip**: Indicador (1/0) de ida y vuelta.
* **trip_country**: Pa√≠s del viaje.
* **trip_market**: Mercado del viaje (Dom√©stico / Regional).
* **country_origin**: Pa√≠s de origen.
* **country_dest**: Pa√≠s de destino.
* **is_international**: Indicador (1/0) de viaje internacional.
* **is_cross_border**: Indicador (1/0) de cruce de fronteras.

---

## üìÖ L√≥gica de Calendario (Bleisure)

* **includes_weekend**: El viaje incluye s√°bado o domingo.
* **only_weekdays**: El viaje ocurre solo en d√≠as h√°biles.
* **tripstart_is_weekend**: La ida es en fin de semana.
* **tripend_is_weekend**: La vuelta es en fin de semana.

---

## üéØ Objetivos / Labels

* **motivo_de_viaje**: Etiqueta real del motivo del viaje (Business / Leisure).
* **motivo_predicho**: Clasificaci√≥n generada por el modelo.

---

Si quieres, en el siguiente paso puedo:

* Convertir esto en **JSON / YAML / Markdown t√©cnico**
* Proponer un **modelo estrella (facts & dimensions)**
* O ayudarte a definir **features finales para ML** (bleisure / churn / revenue)

---

# Referencia Maestra de Par√°metros XGBoost

Este documento detalla los hiperpar√°metros esenciales para la configuraci√≥n, regularizaci√≥n y optimizaci√≥n de modelos XGBoost.

---

### 1. Control de Estructura y Aprendizaje

**`learning_rate` (eta)**:  
Controla la magnitud de la actualizaci√≥n de los pesos en cada paso (shrinkage). Despu√©s de cada paso de boosting, se obtienen los pesos de las nuevas caracter√≠sticas y el `learning_rate` reduce estos pesos para hacer el proceso de boosting m√°s conservador.  
* **Significancia:** Un valor bajo hace que el modelo sea m√°s robusto al sobreajuste (overfitting), pero el entrenamiento ser√° m√°s lento y requerir√° m√°s √°rboles (`n_estimators`). Un valor alto aprende r√°pido pero puede atascarse en √≥ptimos locales o divergir.
* **Rango:** `[0, 1]`. T√≠pico: `0.01 - 0.3`.

**`n_estimators`**:  
El n√∫mero de iteraciones de boosting (n√∫mero de √°rboles a construir).  
* **Significancia:** Si es muy bajo, el modelo no aprender√° lo suficiente (underfitting). Si es excesivamente alto, aumenta el riesgo de overfitting (aunque XGBoost es bastante resistente a esto si el `learning_rate` es bajo). Generalmente, si reduces el `learning_rate`, debes aumentar `n_estimators`.
* **Rango:** `Entero > 0`. T√≠pico: `100 - 5000`.

**`max_depth`**:  
Profundidad m√°xima de cada √°rbol.  
* **Significancia:** Controla la complejidad del modelo. √Årboles profundos pueden capturar relaciones muy espec√≠ficas y patrones de alto orden, pero memorizan el ruido (overfitting). √Årboles poco profundos (stumps) son buenos para capturar tendencias lineales simples.
* **Rango:** `Entero > 0`. T√≠pico: `3 - 10`.

**`min_child_weight`**:  
Define la suma m√≠nima de peso de instancias (Hessian) necesaria en un nodo hijo (hoja).  
* **Significancia:** Es fundamental para controlar el overfitting. Si el paso de partici√≥n del √°rbol resulta en un nodo hoja con una suma de pesos menor que este valor, el proceso de construcci√≥n se detendr√°. En t√©rminos simples: evita que el modelo cree reglas para grupos de datos muy peque√±os o insignificantes. Cuanto mayor sea el valor, m√°s conservador es el modelo.
* **Rango:** `[0, ‚àû]`. T√≠pico: `1 - 10`.

**`gamma` (min_split_loss)**:  
Reducci√≥n m√≠nima de la funci√≥n de p√©rdida requerida para realizar una partici√≥n adicional en un nodo hoja.  
* **Significancia:** Act√∫a como un par√°metro de pseudo-regularizaci√≥n. A diferencia de otros algoritmos que podan el √°rbol despu√©s de construirlo, XGBoost usa `gamma` para no crecer el √°rbol si la ganancia no es sustancial. Valores altos hacen el algoritmo muy conservador.
* **Rango:** `[0, ‚àû]`. T√≠pico: `0 - 5`.

---

### 2. Estocasticidad (Muestreo Aleatorio)

**`subsample`**:  
Fracci√≥n de observaciones (filas) a muestrear aleatoriamente para cada √°rbol.  
* **Significancia:** Previene el overfitting al hacer que cada √°rbol vea un subconjunto diferente de los datos. Si se establece en 0.5, XGBoost recolectar√° aleatoriamente la mitad de los datos para crecer los √°rboles.
* **Rango:** `(0, 1]`. T√≠pico: `0.5 - 0.9`.

**`colsample_bytree`**:  
Fracci√≥n de columnas (features) a muestrear aleatoriamente para cada √°rbol.  
* **Significancia:** Similar a "Random Forest". √ötil cuando tienes muchas caracter√≠sticas o algunas caracter√≠sticas dominantes que opacan a las dem√°s. Obliga al modelo a considerar variables menos potentes.
* **Rango:** `(0, 1]`. T√≠pico: `0.5 - 0.9`.

---

### 3. Regularizaci√≥n Avanzada

**`reg_alpha` (alpha)**:  
T√©rmino de regularizaci√≥n L1 en los pesos.  
* **Significancia:** Promueve la "esparcidad" (sparsity). Esto significa que fuerza los pesos de las caracter√≠sticas menos importantes a ser exactamente cero. Es √∫til para realizar selecci√≥n de caracter√≠sticas (feature selection) impl√≠cita en datasets con mucho ruido o alta dimensionalidad.
* **Rango:** `[0, ‚àû]`. T√≠pico: `0 - 10`.

**`reg_lambda` (lambda)**:  
T√©rmino de regularizaci√≥n L2 en los pesos.  
* **Significancia:** Suaviza los pesos de las hojas, evitando que un solo nodo tenga una influencia desproporcionada. Ayuda a reducir el overfitting de manera m√°s suave que L1. Es la regularizaci√≥n por defecto de XGBoost.
* **Rango:** `[0, ‚àû]`. T√≠pico: `1 - 10`.

---

### 4. Manejo de Desbalance y Estabilidad

**`scale_pos_weight`**:  
Controla el balance de pesos entre clases positivas y negativas.  
* **Significancia:** Cr√≠tico para datasets desbalanceados. Un valor t√≠pico es `sum(negative instances) / sum(positive instances)`. Hace que el modelo penalice mucho m√°s el error al clasificar mal la clase minoritaria (positiva).
* **Rango:** `> 0`.

**`max_delta_step`**:  
Restricci√≥n m√°xima en el paso de actualizaci√≥n de peso (delta) de cada hoja.  
* **Significancia:** Generalmente no es necesario, pero es vital en regresi√≥n log√≠stica con clases extremadamente desbalanceadas. Si el modelo es inestable o el gradiente explota, establecer esto en un valor finito (ej. 1-10) ayuda a la convergencia.
* **Rango:** `[0, ‚àû]`. T√≠pico: `0 (sin l√≠mite)` o `1 - 10`.

---

### 5. Definici√≥n del Problema y Sistema

**`objective`**:  
Especifica la funci√≥n de p√©rdida a minimizar.  
* **Significancia:** Define la naturaleza matem√°tica del problema. Debe coincidir con tu variable objetivo (target). Usar la funci√≥n incorrecta invalidar√° los resultados.
* **Valores:** `String`. Ejemplos: `binary:logistic` (Clasificaci√≥n binaria, devuelve probabilidad), `reg:squarederror` (Regresi√≥n), `multi:softmax` (Multiclase).

**`eval_metric`**:  
M√©trica de evaluaci√≥n para datos de validaci√≥n durante el entrenamiento.  
* **Significancia:** Permite monitorear el rendimiento real del modelo iteraci√≥n tras iteraci√≥n. Fundamental para usar "Early Stopping". Para datos desbalanceados, `error` es malo, `auc` o `logloss` son preferibles.
* **Valores:** `String` o `Lista`. Ejemplos: `auc`, `logloss`, `rmse`, `mae`, `error`.

**`tree_method`**:  
Algoritmo de construcci√≥n del √°rbol.  
* **Significancia:** Afecta dr√°sticamente la velocidad de entrenamiento y el uso de memoria. Para datasets grandes (>100k filas), los m√©todos basados en histogramas son la norma.
* **Valores:** `String`. Ejemplos: `auto`, `exact`, `hist` (r√°pido en CPU), `gpu_hist` (r√°pido en GPU).

**`n_jobs`**:  
N√∫mero de hilos paralelos usados para correr XGBoost.  
* **Significancia:** Solo afecta la velocidad de c√≥mputo, no el resultado del modelo.
* **Rango:** `Entero`. `-1` usa todos los n√∫cleos disponibles.

**`random_state` (seed)**:  
Semilla para el generador de n√∫meros aleatorios.  
* **Significancia:** Garantiza la reproducibilidad. Mismos datos + mismo par√°metro = mismo resultado exacto.
* **Rango:** `Entero`.

---

# Diccionario de Par√°metros y M√©tricas para LightGBM

Este documento describe las m√©tricas de evaluaci√≥n y los hiperpar√°metros comunes utilizados en los modelos LightGBM, bas√°ndose en el contexto de tu notebook.

---

## M√©tricas de Evaluaci√≥n

Estas son m√©tricas que se calculan despu√©s de entrenar el modelo para medir su rendimiento. No son par√°metros que se configuran antes del entrenamiento.

### `ROC-AUC`
*   **Qu√© es**: Una m√©trica de rendimiento para problemas de clasificaci√≥n. Significa "√Årea Bajo la Curva de Caracter√≠stica Operativa del Receptor".
*   **Qu√© hace**: Mide la capacidad del modelo para distinguir entre las clases positiva y negativa. Un valor de 1.0 indica un clasificador perfecto, mientras que un valor de 0.5 sugiere un rendimiento no mejor que el azar.
*   **Valores**: Un n√∫mero flotante entre 0.0 y 1.0.

### `Precision (1)`
*   **Qu√© es**: Una m√©trica que mide la exactitud de las predicciones positivas.
*   **Qu√© hace**: Responde a la pregunta: "De todas las veces que el modelo predijo la clase 1 (positiva), ¬øqu√© porcentaje fue correcto?". Se calcula como `Verdaderos Positivos / (Verdaderos Positivos + Falsos Positivos)`.
*   **Valores**: Un n√∫mero flotante entre 0.0 y 1.0.

### `Recall (1)`
*   **Qu√© es**: Una m√©trica que mide la completitud de las predicciones positivas. Tambi√©n se conoce como "Sensibilidad".
*   **Qu√© hace**: Responde a la pregunta: "De todos los casos que eran realmente de la clase 1, ¬øqu√© porcentaje detect√≥ correctamente el modelo?". Se calcula como `Verdaderos Positivos / (Verdaderos Positivos + Falsos Negativos)`.
*   **Valores**: Un n√∫mero flotante entre 0.0 y 1.0.

### `F1-Score (1)`
*   **Qu√© es**: La media arm√≥nica de `Precision` y `Recall`.
*   **Qu√© hace**: Proporciona un balance entre la precisi√≥n y el recall. Es √∫til cuando se necesita un equilibrio entre identificar correctamente los casos positivos y no generar demasiados falsos positivos.
*   **Valores**: Un n√∫mero flotante entre 0.0 y 1.0.

---

## Hiperpar√°metros del Modelo LightGBM

Estos son ajustes que se configuran *antes* de entrenar el modelo para controlar su comportamiento y rendimiento.

### `objective`
*   **Qu√© es**: El objetivo de aprendizaje.
*   **Qu√© hace**: Define la funci√≥n de p√©rdida que el modelo intentar√° minimizar durante el entrenamiento. Esto le dice al modelo qu√© tipo de problema est√° resolviendo.
*   **Par√°metros que acepta**:
    *   `'binary'`: Para clasificaci√≥n binaria (dos clases).
    *   `'multiclass'`: Para clasificaci√≥n multiclase.
    *   `'regression'`: Para problemas de regresi√≥n.

### `metric`
*   **Qu√© es**: La m√©trica de evaluaci√≥n a monitorear.
*   **Qu√© hace**: Especifica qu√© m√©trica se usar√° para evaluar el rendimiento del modelo en el conjunto de validaci√≥n durante el entrenamiento. Es crucial para t√©cnicas como el `early stopping`.
*   **Par√°metros que acepta**: `'auc'`, `'binary_logloss'`, `'f1'`, `'precision'`, `'recall'`, `'None'` (si no se desea una m√©trica espec√≠fica aqu√≠).

### `n_jobs`
*   **Qu√© es**: El n√∫mero de hilos de CPU a utilizar.
*   **Qu√© hace**: Controla el paralelismo del entrenamiento.
*   **Par√°metros que acepta**:
    *   Un n√∫mero entero (ej. `4` para usar 4 hilos).
    *   `-1`: Para usar todos los hilos de CPU disponibles.

### `verbosity`
*   **Qu√© es**: El nivel de detalle de los mensajes impresos.
*   **Qu√© hace**: Controla cu√°nta informaci√≥n muestra LightGBM durante el entrenamiento.
*   **Par√°metros que acepta**:
    *   `< 0` (ej. `-1`): Muestra solo errores fatales.
    *   `0`: Muestra errores y advertencias.
    *   `1`: Muestra informaci√≥n adicional.

### `random_state`
*   **Qu√© es**: La semilla para la generaci√≥n de n√∫meros aleatorios.
*   **Qu√© hace**: Asegura que los resultados del modelo sean reproducibles. Cualquier operaci√≥n estoc√°stica (como el `subsampling`) producir√° los mismos resultados si se usa la misma semilla.
*   **Par√°metros que acepta**: Un n√∫mero entero (ej. `42`).

### `n_estimators`
*   **Qu√© es**: El n√∫mero de √°rboles de decisi√≥n a construir.
*   **Qu√© hace**: Controla la cantidad de rondas de boosting. Un n√∫mero mayor puede mejorar el rendimiento, pero tambi√©n aumenta el riesgo de sobreajuste y el tiempo de entrenamiento.
*   **Par√°metros que acepta**: Un n√∫mero entero positivo (ej. `100`, `1000`).

### `learning_rate`
*   **Qu√© es**: La tasa de aprendizaje.
*   **Qu√© hace**: Reduce la contribuci√≥n de cada √°rbol nuevo. Un valor m√°s bajo requiere m√°s `n_estimators` pero generalmente conduce a un modelo m√°s robusto y preciso.
*   **Par√°metros que acepta**: Un n√∫mero flotante peque√±o, t√≠picamente entre `0.01` y `0.3`.

### `num_leaves`
*   **Qu√© es**: El n√∫mero m√°ximo de hojas en un √°rbol.
*   **Qu√© hace**: Es el principal par√°metro para controlar la complejidad de un √°rbol individual. Un valor m√°s alto permite al modelo aprender relaciones m√°s complejas, pero aumenta el riesgo de sobreajuste.
*   **Par√°metros que acepta**: Un n√∫mero entero, debe ser menor que `2^max_depth`.

### `max_depth`
*   **Qu√© es**: La profundidad m√°xima de un √°rbol.
*   **Qu√© hace**: Limita la profundidad de cada √°rbol para evitar el sobreajuste. Un valor de `-1` significa sin l√≠mite.
*   **Par√°metros que acepta**: Un n√∫mero entero (ej. `5`, `10`) o `-1`.

### `subsample`
*   **Qu√© es**: La fracci√≥n de datos a usar para entrenar cada √°rbol.
*   **Qu√© hace**: LightGBM tomar√° una muestra aleatoria de las filas (sin reemplazo) antes de construir cada √°rbol. Esto ayuda a prevenir el sobreajuste.
*   **Par√°metros que acepta**: Un n√∫mero flotante entre `0.0` y `1.0` (ej. `0.8` para usar el 80% de los datos).

### `colsample_bytree`
*   **Qu√© es**: La fracci√≥n de caracter√≠sticas (columnas) a usar para entrenar cada √°rbol.
*   **Qu√© hace**: En cada iteraci√≥n, se selecciona un subconjunto aleatorio de caracter√≠sticas. Ayuda a prevenir el sobreajuste y acelera el entrenamiento.
*   **Par√°metros que acepta**: Un n√∫mero flotante entre `0.0` y `1.0` (ej. `0.7` para usar el 70% de las columnas).

### `min_child_samples`
*   **Qu√© es**: El n√∫mero m√≠nimo de muestras de datos requeridas en una hoja.
*   **Qu√© hace**: Evita que el modelo cree divisiones que solo se aplican a muy pocos datos, lo que ayuda a controlar el sobreajuste.
*   **Par√°metros que acepta**: Un n√∫mero entero positivo (ej. `20`).

### `scale_pos_weight`
*   **Qu√© es**: El peso para la clase positiva.
*   **Qu√© hace**: Se utiliza en problemas con clases desbalanceadas. Aumenta el peso de la clase minoritaria (positiva) en la funci√≥n de p√©rdida, haciendo que el modelo preste m√°s atenci√≥n a los errores en esa clase.
*   **Par√°metros que acepta**: Un n√∫mero flotante. Un valor com√∫n es `(n√∫mero de muestras negativas) / (n√∫mero de muestras positivas)`.

